{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from cosmosqa.data_loader.dataloader import create_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmosqa.data_loader.dataset import CosmosQADataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "MAX_LEN = 160\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train_sample.csv\")\n",
    "df_valid = pd.read_csv(\"../data/valid_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(\n",
    "    df=df_train, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE\n",
    ")\n",
    "valid_data_loader = create_data_loader(\n",
    "    df=df_valid, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT with multiway attention\n",
    "class BertMultiwayMatch(nn.Module):\n",
    "    def __init__(self, config, num_choices=4):\n",
    "        super(BertMultiwayMatch, self).__init__()\n",
    "        self.num_choices = num_choices\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.linear_trans = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.linear_fuse_p = nn.Linear(\n",
    "            config.hidden_size * 2, config.hidden_size\n",
    "        )\n",
    "        self.linear_fuse_q = nn.Linear(\n",
    "            config.hidden_size * 2, config.hidden_size\n",
    "        )\n",
    "        self.linear_fuse_a = nn.Linear(\n",
    "            config.hidden_size * 2, config.hidden_size\n",
    "        )\n",
    "        self.classifier = nn.Linear(config.hidden_size * 3, 1)\n",
    "\n",
    "    def matching(\n",
    "        self,\n",
    "        passage_encoded,\n",
    "        question_encoded,\n",
    "        passage_attention_mask,\n",
    "        question_attention_mask,\n",
    "    ):\n",
    "        # linear trans the other way\n",
    "        passage_encoded_trans = self.linear_trans(passage_encoded)\n",
    "        question_encoded_trans = self.linear_trans(question_encoded)\n",
    "        p2q_scores = torch.matmul(\n",
    "            passage_encoded_trans, question_encoded_trans.transpose(2, 1)\n",
    "        )\n",
    "\n",
    "        # fp16 compatibility\n",
    "        merged_attention_mask = (\n",
    "            passage_attention_mask.unsqueeze(2)\n",
    "            .float()\n",
    "            .matmul(question_attention_mask.unsqueeze(1).float())\n",
    "        )\n",
    "        merged_attention_mask = merged_attention_mask.to(\n",
    "            dtype=next(self.parameters()).dtype\n",
    "        )\n",
    "        merged_attention_mask = (1.0 - merged_attention_mask) * -10000.0\n",
    "\n",
    "        p2q_scores_ = p2q_scores + merged_attention_mask\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        p2q_w = nn.Softmax(dim=-1)(p2q_scores_)\n",
    "        p2q_w_ = nn.Softmax(dim=1)(p2q_scores_)\n",
    "\n",
    "        # question attentive passage representation\n",
    "        mp = torch.matmul(p2q_w, question_encoded)\n",
    "        # passage attentive question representation\n",
    "        mq = torch.matmul(p2q_w_.transpose(2, 1), passage_encoded)\n",
    "\n",
    "        return mp, mq\n",
    "\n",
    "    # sub and multiply\n",
    "    def fusing_mlp(\n",
    "        self,\n",
    "        passage_encoded,\n",
    "        mp_q,\n",
    "        mp_a,\n",
    "        mp_qa,\n",
    "        question_encoded,\n",
    "        mq_p,\n",
    "        mq_a,\n",
    "        mq_pa,\n",
    "        answers_encoded,\n",
    "        ma_p,\n",
    "        ma_q,\n",
    "        ma_pq,\n",
    "    ):\n",
    "        new_mp_q = torch.cat(\n",
    "            [mp_q - passage_encoded, mp_q * passage_encoded], 2\n",
    "        )\n",
    "        new_mp_a = torch.cat(\n",
    "            [mp_a - passage_encoded, mp_a * passage_encoded], 2\n",
    "        )\n",
    "        new_mp_qa = torch.cat(\n",
    "            [mp_qa - passage_encoded, mp_qa * passage_encoded], 2\n",
    "        )\n",
    "        new_mq_p = torch.cat(\n",
    "            [mq_p - question_encoded, mq_p * question_encoded], 2\n",
    "        )\n",
    "        new_mq_a = torch.cat(\n",
    "            [mq_a - question_encoded, mq_a * question_encoded], 2\n",
    "        )\n",
    "        new_mq_pa = torch.cat(\n",
    "            [mq_pa - question_encoded, mq_pa * question_encoded], 2\n",
    "        )\n",
    "        new_ma_p = torch.cat(\n",
    "            [ma_p - answers_encoded, ma_p * answers_encoded], 2\n",
    "        )\n",
    "        new_ma_q = torch.cat(\n",
    "            [ma_q - answers_encoded, ma_q * answers_encoded], 2\n",
    "        )\n",
    "        new_ma_pq = torch.cat(\n",
    "            [ma_pq - answers_encoded, ma_pq * answers_encoded], 2\n",
    "        )\n",
    "\n",
    "        new_mp = torch.cat([new_mp_q, new_mp_a, new_mp_qa], 1)\n",
    "        new_mq = torch.cat([new_mq_p, new_mq_a, new_mq_pa], 1)\n",
    "        new_ma = torch.cat([new_ma_p, new_ma_q, new_ma_pq], 1)\n",
    "\n",
    "        # use separate linear functions\n",
    "        new_mp_ = F.relu(self.linear_fuse_p(new_mp))\n",
    "        new_mq_ = F.relu(self.linear_fuse_q(new_mq))\n",
    "        new_ma_ = F.relu(self.linear_fuse_a(new_ma))\n",
    "\n",
    "        new_p_max, new_p_idx = torch.max(new_mp_, 1)\n",
    "        new_q_max, new_q_idx = torch.max(new_mq_, 1)\n",
    "        new_a_max, new_a_idx = torch.max(new_ma_, 1)\n",
    "\n",
    "        new_p_max_ = new_p_max.view(-1, self.num_choices, new_p_max.size(1))\n",
    "        new_q_max_ = new_q_max.view(-1, self.num_choices, new_q_max.size(1))\n",
    "        new_a_max_ = new_a_max.view(-1, self.num_choices, new_a_max.size(1))\n",
    "\n",
    "        c = torch.cat([new_p_max_, new_q_max_, new_a_max_], 2)\n",
    "\n",
    "        return c\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        token_type_ids=None,\n",
    "        attention_mask=None,\n",
    "        doc_len=None,\n",
    "        ques_len=None,\n",
    "        option_len=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "#         flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "#         doc_len = doc_len.view(-1, doc_len.size(0) * doc_len.size(1)).squeeze()\n",
    "#         ques_len = ques_len.view(\n",
    "#             -1, ques_len.size(0) * ques_len.size(1)\n",
    "#         ).squeeze()\n",
    "#         option_len = option_len.view(\n",
    "#             -1, option_len.size(0) * option_len.size(1)\n",
    "#         ).squeeze()\n",
    "#         flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
    "#         flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
    "\n",
    "#         sequence_output, pooled_output = self.bert.forward(\n",
    "#             flat_input_ids,\n",
    "#             flat_token_type_ids,\n",
    "#             flat_attention_mask,\n",
    "#             output_all_encoded_layers=False,\n",
    "#         )\n",
    "        print(\"input_ids\")\n",
    "        print(input_ids.size())\n",
    "        print(\"doc_len\")\n",
    "        print(doc_len.size())\n",
    "        print(\"ques_len\")\n",
    "        print(ques_len.size())\n",
    "        print(\"option_len\")\n",
    "        print(option_len.size())\n",
    "        print(\"labels\")\n",
    "        print(labels.size())\n",
    "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        print(\"flat_input_ids\")\n",
    "        print(flat_input_ids.size())\n",
    "        doc_len = doc_len.view(-1, doc_len.size(0) * doc_len.size(1)).squeeze()\n",
    "        print(\"doc_len_squeezed\")\n",
    "        print(doc_len.size())\n",
    "        ques_len = ques_len.view(-1, ques_len.size(0) * ques_len.size(1)).squeeze()\n",
    "        print(\"ques_len_squeezed\")\n",
    "        print(ques_len.size())\n",
    "        option_len = option_len.view(-1, option_len.size(0) * option_len.size(1)).squeeze()\n",
    "        print(\"option_len_squeezed\")\n",
    "        print(option_len.size())\n",
    "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n",
    "        print(\"flat_token_type_ids\")\n",
    "        print(flat_token_type_ids.size())\n",
    "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
    "        print(\"flat_attention_mask\")\n",
    "        print(flat_attention_mask.size())\n",
    "\n",
    "        sequence_output, pooled_output = self.bert.forward(flat_input_ids, flat_token_type_ids,flat_attention_mask)\n",
    "\n",
    "        print(\"sequence_output\")\n",
    "        print(sequence_output.size())\n",
    "        print(\"pooled_output\")\n",
    "        print(pooled_output.size())\n",
    "        (\n",
    "            passage_encoded,\n",
    "            question_encoded,\n",
    "            answers_encoded,\n",
    "            passage_question_encoded,\n",
    "            passage_answer_encoded,\n",
    "            question_answer_encoded,\n",
    "        ) = seperate_seq(sequence_output, doc_len, ques_len, option_len)\n",
    "        print(\"passage_encoded\")\n",
    "        print(passage_encoded.size())\n",
    "        print(\"passage_question_encoded\")\n",
    "        print(passage_question_encoded.size())\n",
    "        (\n",
    "            passage_attention_mask,\n",
    "            question_attention_mask,\n",
    "            answers_attention_mask,\n",
    "            passage_question_attention_mask,\n",
    "            passage_answer_attention_mask,\n",
    "            question_answer_attention_mask,\n",
    "        ) = seperate_seq_attention(\n",
    "            flat_attention_mask, doc_len, ques_len, option_len\n",
    "        )\n",
    "\n",
    "        # matching layer\n",
    "        mp_q, mq_p = self.matching(\n",
    "            passage_encoded,\n",
    "            question_encoded,\n",
    "            passage_attention_mask,\n",
    "            question_attention_mask,\n",
    "        )\n",
    "        print(\"mp_q\")\n",
    "        print(mp_q.size())\n",
    "        mp_a, ma_p = self.matching(\n",
    "            passage_encoded,\n",
    "            answers_encoded,\n",
    "            passage_attention_mask,\n",
    "            answers_attention_mask,\n",
    "        )\n",
    "        mp_qa, mqa_p = self.matching(\n",
    "            passage_encoded,\n",
    "            question_answer_encoded,\n",
    "            passage_attention_mask,\n",
    "            question_answer_attention_mask,\n",
    "        )\n",
    "        mq_a, ma_q = self.matching(\n",
    "            question_encoded,\n",
    "            answers_encoded,\n",
    "            question_attention_mask,\n",
    "            answers_attention_mask,\n",
    "        )\n",
    "        mq_pa, mpa_q = self.matching(\n",
    "            question_encoded,\n",
    "            passage_answer_encoded,\n",
    "            question_attention_mask,\n",
    "            passage_answer_attention_mask,\n",
    "        )\n",
    "        ma_pq, mpq_a = self.matching(\n",
    "            answers_encoded,\n",
    "            passage_question_encoded,\n",
    "            answers_attention_mask,\n",
    "            passage_question_attention_mask,\n",
    "        )\n",
    "\n",
    "        # MLP fuse\n",
    "        c = self.fusing_mlp(\n",
    "            passage_encoded,\n",
    "            mp_q,\n",
    "            mp_a,\n",
    "            mp_qa,\n",
    "            question_encoded,\n",
    "            mq_p,\n",
    "            mq_a,\n",
    "            mq_pa,\n",
    "            answers_encoded,\n",
    "            ma_p,\n",
    "            ma_q,\n",
    "            ma_pq,\n",
    "        )\n",
    "        c_ = c.view(-1, c.size(2))\n",
    "        logits = self.classifier(c_)\n",
    "        reshaped_logits = logits.view(-1, self.num_choices)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "            return loss, reshaped_logits\n",
    "        else:\n",
    "            return reshaped_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 1024\n",
    "}\n",
    "config = dotdict(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertMultiwayMatch(config=config, num_choices=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_data_loader:\n",
    "    input_ids = d[\"input_ids\"]\n",
    "    attention_mask = d[\"attention_mask\"]\n",
    "    token_type_ids = d[\"token_type_ids\"]\n",
    "    c_len = d[\"c_len\"]\n",
    "    q_len = d[\"q_len\"]\n",
    "    a_len = d[\"a_len\"]\n",
    "    targets = d[\"target\"].type(dtype=torch.long)\n",
    "\n",
    "    loss, logits = model(\n",
    "        input_ids=input_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        doc_len=c_len,\n",
    "        ques_len=q_len,\n",
    "        option_len=a_len,\n",
    "        labels=targets,\n",
    "    )\n",
    "    print(\"loss\")\n",
    "    print(loss)\n",
    "    print(\"logits\")\n",
    "    print(logits)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
